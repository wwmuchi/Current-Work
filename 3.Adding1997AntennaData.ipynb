{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing eng_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "file_path = 'Imports/Station Data/baps1997/fmfxeng.dat'\n",
    "\n",
    "variable_lengths = {\n",
    "    \"Mainkey\": 16, \"Channel\": 3, \"Service Class\": 2, \"Country\": 2,\n",
    "    \"State\": 2, \"City\": 50, \"Frequency\": 5, \"Call Sign\": 12,\n",
    "    \"Latitude Indicator\": 1, \"Latitude Degrees\": 2, \"Latitude Minutes\": 2, \"Latitude Seconds\": 2,\n",
    "    \"Longitude Ind.\": 1, \"Longitude Degrees\": 3, \"Longitude Minutes\": 2, \"Longitude Seconds\": 2,\n",
    "    \"Domestic Status\": 6, \"Internat. Status\": 6, \"Coord. Status\": 1, \"File No. Prefix\": 6,\n",
    "    \"App Ref No (ARN)\": 8, \"Class\": 2, \"Border\": 1, \"Border Dist.\": 3,\n",
    "    \"Horizontal ERP\": 4, \"Horiz. Max ERP\": 4, \"Horizontal HAAT\": 5, \"Horiz RCAMSL\": 5,\n",
    "    \"Vertical ERP\": 4, \"Vert. Max ERP\": 4, \"Vertical HAAT\": 5, \"Vert RCAMSL\": 5,\n",
    "    \"Maximum HAAT\": 5, \"Beam Tilt\": 1, \"Directional Ant\": 1, \"Docket No.\": 8,\n",
    "    \"Owner\": 50, \"Filler (1)\": 2, \"Cutoff Date\": 6, \"Filler (2)\": 2,\n",
    "    \"Window Open Date\": 6, \"Antenna Make\": 3, \"Antenna Model\": 14, \"Rotate\": 5,\n",
    "    \"Filler (3)\": 2, \"Last Notify Date\": 6, \"Last Notify Time\": 6, \"Filler (4)\": 2,\n",
    "    \"Last Update Date\": 6, \"Last Update Time\": 6, \"Is it 73.215\": 1, \"Filler (5)\": 2,\n",
    "    \"CP Expire Date\": 6, \"Filler (6)\": 2, \"Filed Lic Date\": 6, \"Filler (7)\": 2,\n",
    "    \"Filed 307 Date\": 6, \"Internat. Class\": 2, \"Horiz. RCAGL\": 4, \"Vert. RCAGL\": 4\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "eng_data = pd.read_fwf(file_path, widths=variable_lengths.values(), names=variable_lengths.keys())\n",
    "\n",
    "eng_data = eng_data.drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert Date columns to datetime\n",
    "for col in eng_data.columns:\n",
    "    if 'Date' in col:\n",
    "        converted_col = pd.to_datetime(eng_data[col].copy(), format='%y%m%d')\n",
    "        eng_data[f'{col} DT'] = converted_col\n",
    "        eng_data.drop(columns=[col], inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'year_count': 2150, 'month_count': 9752, 'day_count': 8495}\n",
      "25647\n"
     ]
    }
   ],
   "source": [
    "# Is there a relationship b/w the last update date and the mainkey?\n",
    "eng_data[['Last Update Date DT', 'Mainkey']]\n",
    "\n",
    "count = {'year_count' : 0, 'month_count' : 0, 'day_count' : 0}\n",
    "\n",
    "for index, row in eng_data[['Last Update Date DT', 'Mainkey']].iterrows():\n",
    "\n",
    "    mainkey = row['Mainkey']\n",
    "    year = row['Last Update Date DT'].year % 100\n",
    "    month = row['Last Update Date DT'].month\n",
    "    day = row['Last Update Date DT'].day\n",
    "\n",
    "    for time in [(year, 'year_count'), (month, 'month_count'), (day, 'day_count')]:\n",
    "        if str(time[0]) in mainkey:\n",
    "            count[time[1]] += 1\n",
    "\n",
    "print(count)\n",
    "print(len(eng_data))\n",
    "\n",
    "# Not really"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose columns to keep\n",
    "col_names = [\n",
    "    'Service Class',\n",
    "    'Country',\n",
    "    'State',\n",
    "    'City',\n",
    "    'Call Sign',\n",
    "    'Frequency',\n",
    "    'Domestic Status',\n",
    "    'File No. Prefix',\n",
    "    'Last Update Date DT'\n",
    "]\n",
    "\n",
    "col_keywords = [\n",
    "    'Latitude',\n",
    "    'Longitude',\n",
    "    'ERP',\n",
    "    'RCAMSL',\n",
    "]\n",
    "\n",
    "cols_to_keep = col_names + [col for col in eng_data.columns if any(keyword in col for keyword in col_keywords)]\n",
    "\n",
    "assert all([col in eng_data.columns for col in cols_to_keep])\n",
    "\n",
    "eng_data = eng_data[cols_to_keep]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep license rows\n",
    "eng_data = eng_data[eng_data['Domestic Status'] == 'LIC']\n",
    "eng_data.drop(columns=['Domestic Status'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting latitude and longitude to decimal\n",
    "def to_decimal(degrees, minutes, seconds, direction):\n",
    "    decimal = degrees + (minutes / 60) + (seconds / 3600)\n",
    "    if direction in ['S', 'W']:\n",
    "        decimal = -decimal\n",
    "    return decimal\n",
    "\n",
    "\n",
    "eng_data['Latitude Decimal'] = eng_data.apply(lambda row: to_decimal(row['Latitude Degrees'], row['Latitude Minutes'], row['Latitude Seconds'], row['Latitude Indicator']), axis=1)\n",
    "eng_data['Longitude Decimal'] = eng_data.apply(lambda row: to_decimal(row['Longitude Degrees'], row['Longitude Minutes'], row['Longitude Seconds'], row['Longitude Ind.']), axis=1)\n",
    "\n",
    "lat_lon_cols_to_drop = [col for col in eng_data.columns if 'Latitude' in col or 'Longitude' in col]\n",
    "lat_lon_cols_to_drop = [col for col in lat_lon_cols_to_drop if 'Decimal' not in col]\n",
    "eng_data = eng_data.drop(columns = lat_lon_cols_to_drop )\n",
    "\n",
    "# Round so info can be fed into API\n",
    "eng_data = eng_data.rename(columns = {'Latitude Decimal': 'Latitude', 'Longitude Decimal' : 'Longitude'})\n",
    "eng_data[['Latitude', 'Longitude']] = eng_data[['Latitude', 'Longitude']].map(lambda x: round(x, 10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Which ERP to enter?\n",
    "erp_data = eng_data[[col for col in eng_data.columns if 'ERP' in col]]\n",
    "for col in erp_data.columns:\n",
    "    # print(col)\n",
    "    # print(erp_data[col].isna().sum())\n",
    "    # print(erp_data[col].notna().sum())\n",
    "    pass\n",
    "\n",
    "# Horizontal ERP has the most data\n",
    "\n",
    "# Which RCAMSL to enter?\n",
    "rcamsl_data = eng_data[[col for col in eng_data.columns if 'RCAMSL' in col]]\n",
    "\n",
    "## Apparently vertical ERP is important for aviation, not radio stations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data = eng_data.drop(columns = [col for col in eng_data.columns if 'Vert' in col or 'Max' in col])\n",
    "eng_data.rename(columns = {'Horizontal ERP': 'ERP', 'Horiz RCAMSL': 'RCAMSL'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols dropped: ['Country']\n"
     ]
    }
   ],
   "source": [
    "# Keeping relevant columns and ensuring info in rows\n",
    "assert len(eng_data.query('`Call Sign`.isna() or Frequency.isna()')) == 0\n",
    "print('cols dropped:', [col for col in eng_data.columns if eng_data[col].isna().all() or eng_data[col].nunique() == 1])\n",
    "eng_data = eng_data[[col for col in eng_data.columns if eng_data[col].notna().any() and eng_data[col].nunique() > 1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I will drop rows w missing antenna info: ['ERP', 'RCAMSL', 'Latitude', 'Longitude']\n",
      "number of rows dropped: 601\n",
      "portion dropped: 5.78%\n"
     ]
    }
   ],
   "source": [
    "antenna_cols = eng_data.iloc[:,7:]\n",
    "print(f'I will drop rows w missing antenna info: {list(antenna_cols.columns)}')\n",
    "rows_to_drop = antenna_cols.isna().any(axis=1)\n",
    "print(f'number of rows dropped: {rows_to_drop.sum()}')\n",
    "print(f'portion dropped: {rows_to_drop.sum()/len(eng_data):.2%}')\n",
    "eng_data = eng_data[~rows_to_drop]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Dealing w/ duplicate call signs***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "portion of duplicates: 4.37%\n"
     ]
    }
   ],
   "source": [
    "eng_data_duplicates = eng_data[eng_data['Call Sign'].duplicated(keep=False)].sort_values(by='Call Sign')\n",
    "print(f'portion of duplicates: {len(eng_data_duplicates)/len(eng_data):.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "File No. Prefix\n",
    "| Prefix  | Description                                                        |\n",
    "|---------|--------------------------------------------------------------------|\n",
    "| BMLH    | Broadcast License Modification for a commercial FM station (High Power) |\n",
    "| BLH     | Broadcast License for a commercial FM station (High Power)        |\n",
    "| BMLED   | Broadcast License Modification for an educational FM station (Low Power) |\n",
    "| BLED    | Broadcast License for an educational FM station (Low Power)       |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "File No. Prefix\n",
       "{BLH}            136\n",
       "{BLH, BMLH}       60\n",
       "{BLED}             8\n",
       "{BMLED, BLED}      6\n",
       "{BMLED}            1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_data_duplicates.groupby('Call Sign')['File No. Prefix'].apply(lambda x: set(x)).value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Service Class\n",
    "| Code | Description                                                                                   |\n",
    "|------|-----------------------------------------------------------------------------------------------|\n",
    "| FA   | A vacant or used channel in the Table of Allotments.                                          |\n",
    "| FM   | A full-service FM station or application.                                                     |\n",
    "| FS   | A full-service FM station auxiliary transmitting antenna or application.                      |\n",
    "| FX   | A translator or application for a translator.                                                 |\n",
    "| FR   | A proposed rule making to amend the Table of Allotments.                                      |\n",
    "| FB   | A booster station or application for a booster.                                               |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Service Class\n",
       "{FM, FS}    203\n",
       "{FM}          8\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eng_data_duplicates.groupby('Call Sign')['Service Class'].apply(lambda x: set(x)).value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices_to_drop_list = []\n",
    "indices_to_keep_list = []\n",
    "indices_processed = []\n",
    "\n",
    "\n",
    "def process_double_rows(df):\n",
    "    assert len(df) == 2\n",
    "\n",
    "    if df['Last Update Date DT'].nunique() == 1 and df['File No. Prefix'].nunique() > 1: # if the dates are the same and file type diff\n",
    "        mask = df['File No. Prefix'].str.contains('M')\n",
    "        indices_to_drop = df[mask].index\n",
    "        indices_to_keep = df[~mask].index\n",
    "        assert len(indices_to_drop) == 1 and len(indices_to_keep) == 1\n",
    "        indices_to_drop_list.extend(indices_to_drop)\n",
    "        indices_to_keep_list.extend(indices_to_keep)\n",
    "        indices_processed.extend(df.index)\n",
    "\n",
    "\n",
    "    elif df['Last Update Date DT'].nunique() > 1: # if dates are different and file type is the same\n",
    "        mask = df['Last Update Date DT'] == df['Last Update Date DT'].max()\n",
    "        indices_to_keep = df[mask].index\n",
    "        indices_to_drop = df[~mask].index\n",
    "        assert len(indices_to_drop) == 1 and len(indices_to_keep) == 1\n",
    "        indices_to_drop_list.extend(indices_to_drop)\n",
    "        indices_to_keep_list.extend(indices_to_keep)\n",
    "        indices_processed.extend(df.index)\n",
    "    \n",
    "    else:\n",
    "        assert 1 == 2\n",
    "\n",
    "\n",
    "for name, df in eng_data_duplicates.groupby('Call Sign'):\n",
    "\n",
    "    if set(df['Service Class']) == {'FM'}: # FM service class only\n",
    "        process_double_rows(df)\n",
    "\n",
    "    elif set(df['Service Class']) == {'FM', 'FS'}:\n",
    "\n",
    "        if len(df) == 2:\n",
    "            indices_to_keep_list.extend(df.index)\n",
    "            indices_processed.extend(df.index)\n",
    "\n",
    "        if len(df) == 3:\n",
    "\n",
    "            assert sorted(df['Service Class']) == ['FM', 'FS', 'FS']\n",
    "            \n",
    "            fm_df = df.query(\"`Service Class` == 'FM'\")\n",
    "            indices_to_keep_list.extend(fm_df.index)\n",
    "            indices_processed.extend(fm_df.index)\n",
    "\n",
    "\n",
    "            fs_df = df.query(\"`Service Class` == 'FS'\")\n",
    "\n",
    "\n",
    "            if (fs_df[['File No. Prefix', 'Last Update Date DT']].nunique() == 1).all(): # If FS rows are the same\n",
    "                indices_to_keep_list.extend(fs_df.index)\n",
    "                indices_processed.extend(fs_df.index)\n",
    "            else:\n",
    "                process_double_rows(fs_df)\n",
    "\n",
    "        if len(df) > 3 or len(df) == 1:\n",
    "            assert 1 == 2 # ensure all cases are covered\n",
    "\n",
    "    else:\n",
    "        assert 1==2\n",
    "\n",
    "\n",
    "assert sorted(indices_processed) == sorted(eng_data_duplicates.index.tolist())\n",
    "assert len(indices_to_drop_list) + len(indices_to_keep_list) == len(indices_processed) == len(eng_data_duplicates)\n",
    "assert pd.Series(indices_to_drop_list).isin(indices_to_keep_list).sum() == 0\n",
    "\n",
    "eng_data = eng_data.drop(index=indices_to_drop_list).copy()\n",
    "eng_data = eng_data.drop(columns=['Service Class', 'File No. Prefix', 'Last Update Date DT'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, all rows with duplicate call signs in eng_data are refer to multiple antennas broadcasting the same signal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Is the relationship b/w call sign and (frequency, city, state) bijective?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data['Tuple'] = eng_data.apply(lambda row: tuple(row[['Frequency', 'City', 'State']]), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numb_uniq_call_signs_per_tuple\n",
       "1    9739\n",
       "2      45\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many call signs are there per tuple?\n",
    "eng_data['numb_uniq_call_signs_per_tuple'] = eng_data.groupby('Tuple')['Call Sign'].transform('nunique')\n",
    "eng_data['tuple_group_size'] = eng_data.groupby('Tuple')['Call Sign'].transform('size')\n",
    "eng_data['numb_uniq_call_signs_per_tuple'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "numb_uniq_tuples_per_callsign\n",
       "1    9784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# How many call signs are there per tuple?\n",
    "eng_data['numb_uniq_tuples_per_callsign'] = eng_data.groupby('Call Sign')['Tuple'].transform('nunique')\n",
    "eng_data['callsign_group_size'] = eng_data.groupby('Call Sign')['Tuple'].transform('size')\n",
    "eng_data['numb_uniq_tuples_per_callsign'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every call sign has a unique tuple.\n",
    "\n",
    "Not every tuple has a unique call sign."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below are the 45 rows of tuples that have multiple call signs:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Call Sign</th>\n",
       "      <th>Frequency</th>\n",
       "      <th>ERP</th>\n",
       "      <th>RCAMSL</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Tuple</th>\n",
       "      <th>numb_uniq_call_signs_per_tuple</th>\n",
       "      <th>tuple_group_size</th>\n",
       "      <th>numb_uniq_tuples_per_callsign</th>\n",
       "      <th>callsign_group_size</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>WSSD</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0.01</td>\n",
       "      <td>209.0</td>\n",
       "      <td>41.728889</td>\n",
       "      <td>-87.550833</td>\n",
       "      <td>(88.1, Chicago, IL)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>330</th>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>WCRX</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0.10</td>\n",
       "      <td>226.0</td>\n",
       "      <td>41.872778</td>\n",
       "      <td>-87.647778</td>\n",
       "      <td>(88.1, Chicago, IL)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>TN</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>W201AP</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0.06</td>\n",
       "      <td>167.0</td>\n",
       "      <td>35.660556</td>\n",
       "      <td>-88.858333</td>\n",
       "      <td>(88.1, Jackson, TN)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>172</th>\n",
       "      <td>TN</td>\n",
       "      <td>Jackson</td>\n",
       "      <td>WAMP</td>\n",
       "      <td>88.1</td>\n",
       "      <td>0.75</td>\n",
       "      <td>166.0</td>\n",
       "      <td>35.660556</td>\n",
       "      <td>-88.858333</td>\n",
       "      <td>(88.1, Jackson, TN)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>IL</td>\n",
       "      <td>Chicago</td>\n",
       "      <td>WZRD</td>\n",
       "      <td>88.3</td>\n",
       "      <td>0.10</td>\n",
       "      <td>209.0</td>\n",
       "      <td>41.982222</td>\n",
       "      <td>-87.718611</td>\n",
       "      <td>(88.3, Chicago, IL)</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    State     City Call Sign  Frequency   ERP  RCAMSL   Latitude  Longitude  \\\n",
       "166    IL  Chicago      WSSD       88.1  0.01   209.0  41.728889 -87.550833   \n",
       "330    IL  Chicago      WCRX       88.1  0.10   226.0  41.872778 -87.647778   \n",
       "47     TN  Jackson    W201AP       88.1  0.06   167.0  35.660556 -88.858333   \n",
       "172    TN  Jackson      WAMP       88.1  0.75   166.0  35.660556 -88.858333   \n",
       "400    IL  Chicago      WZRD       88.3  0.10   209.0  41.982222 -87.718611   \n",
       "\n",
       "                   Tuple  numb_uniq_call_signs_per_tuple  tuple_group_size  \\\n",
       "166  (88.1, Chicago, IL)                               2                 2   \n",
       "330  (88.1, Chicago, IL)                               2                 2   \n",
       "47   (88.1, Jackson, TN)                               2                 2   \n",
       "172  (88.1, Jackson, TN)                               2                 2   \n",
       "400  (88.3, Chicago, IL)                               2                 2   \n",
       "\n",
       "     numb_uniq_tuples_per_callsign  callsign_group_size  \n",
       "166                              1                    1  \n",
       "330                              1                    1  \n",
       "47                               1                    1  \n",
       "172                              1                    1  \n",
       "400                              1                    1  "
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dup_call_sign_per_tup = eng_data.query('numb_uniq_call_signs_per_tuple > 1').sort_values('Tuple')\n",
    "# cols_w_unique_info = (dup_call_sign_per_tup.groupby('Tuple').apply(lambda x: x.nunique(), include_groups=False) > 1).any(axis=0)\n",
    "# dup_call_sign_per_tup = dup_call_sign_per_tup[['Tuple'] + [col for col in cols_w_unique_info.index if cols_w_unique_info[col]]]\n",
    "print(f'Below are the {len(dup_call_sign_per_tup)} rows of tuples that have multiple call signs:')\n",
    "dup_call_sign_per_tup.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of unique (Call Sign, tuple): (9577, 9555)\n",
      "difference = 22\n"
     ]
    }
   ],
   "source": [
    "nuniq_call_sign = eng_data['Call Sign'].nunique()\n",
    "nuniq_tuple = eng_data['Tuple'].nunique()\n",
    "print(f'Number of unique (Call Sign, tuple): {nuniq_call_sign, nuniq_tuple}')\n",
    "print(f'difference = {nuniq_call_sign - nuniq_tuple}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is almost bijective.\n",
    "\n",
    "The mapping is bijective. Except for 22 additional call signs that map onto the same tuples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['numb_uniq_call_signs_per_tuple', 'tuple_group_size', 'numb_uniq_tuples_per_callsign', 'callsign_group_size']\n"
     ]
    }
   ],
   "source": [
    "cols_to_drop = [col for col in eng_data.columns if 'numb_uniq' in col or 'group_size' in col]\n",
    "print(cols_to_drop)\n",
    "eng_data.drop(columns=cols_to_drop, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Match stations in station_data and eng_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/v8/b7w7kd2d6b1_2h62t5lmdqdm0000gn/T/ipykernel_32504/1001790194.py:2: DtypeWarning: Columns (16,18,20,23,24,25) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  station_data_full = pd.read_csv('Exports/Data/2.StationData.csv', index_col=0)\n"
     ]
    }
   ],
   "source": [
    "# Import StationData\n",
    "station_data_full = pd.read_csv('Exports/Data/2.StationData.csv', index_col=0)\n",
    "station_data = station_data_full.query('broadcast == \"FM\" and year == 1998 and market != \"Puerto Rico\" ').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cols dropped: ['stat_broadcast', 'stat_year', 'stat_homestate', 'stat_hometown', 'stat_hours1', 'stat_hours2', 'stat_hours3', 'stat_format4', 'stat_hours4', 'stat_format5', 'stat_hours5', 'stat_format6', 'stat_hours6', 'stat_format7', 'stat_hours7', 'stat_format8', 'stat_note']\n"
     ]
    }
   ],
   "source": [
    "station_data.columns = [f'stat_{col.lower().replace(\" \", \"_\")}' for col in station_data.columns]\n",
    "eng_data.columns = [f'eng_{col.lower().replace(\" \", \"_\")}' for col in eng_data.columns]\n",
    "\n",
    "# Keeping relevant columns\n",
    "assert len(station_data.query('stat_letters.isna() or stat_frequency.isna()')) == 0\n",
    "cols_to_drop = [col for col in station_data.columns if station_data[col].isna().all() or station_data[col].nunique() == 1]\n",
    "print('cols dropped:', cols_to_drop)\n",
    "station_data = station_data[[col for col in station_data.columns if col not in cols_to_drop]].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5617, 9784)"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(station_data), len(eng_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will assume that station_data is the source of truth of 1997 stations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How to identify stations in eng_data?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of duplicate ['eng_call_sign']: 2.12%\n",
      "proportion of duplicate ['eng_call_sign', 'eng_frequency']: 2.12%\n",
      "proportion of duplicate ['eng_city', 'eng_state', 'eng_frequency']: 2.34%\n",
      "proportion of duplicate ['eng_frequency', 'eng_latitude', 'eng_longitude']: 1.13%\n"
     ]
    }
   ],
   "source": [
    "for cols_to_keep in [['eng_call_sign'], ['eng_call_sign', 'eng_frequency'], ['eng_city', 'eng_state', 'eng_frequency'], ['eng_frequency','eng_latitude', 'eng_longitude']]:\n",
    "    eng_view1 = eng_data[cols_to_keep]\n",
    "    eng_view1 = eng_view1[eng_view1.duplicated(keep='first')].sort_values(by=cols_to_keep)\n",
    "    print(f'proportion of duplicate {cols_to_keep}: {len(eng_view1) / len(eng_data):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each call sign has a unique frequency\n",
    "assert (eng_data.groupby('eng_call_sign')['eng_frequency'].apply(lambda x: x.nunique()) == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***How to identify stations in station data?***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "proportion of duplicate ['stat_biastationcode']: 0.0\n",
      "proportion of duplicate ['stat_letters']: 0.0\n"
     ]
    }
   ],
   "source": [
    "for cols_to_keep in [['stat_biastationcode'], ['stat_letters']]:\n",
    "    station_view1 = station_data[cols_to_keep]\n",
    "    station_view1 = station_view1[station_view1.duplicated(keep='first')].sort_values(by=cols_to_keep)\n",
    "    print(f'proportion of duplicate {cols_to_keep}: {len(station_view1) / len(eng_data)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of duplicate ['stat_frequency', 'stat_market'] (excluding \"Not Applicable\"): 0.62%\n"
     ]
    }
   ],
   "source": [
    "cols_to_keep = ['stat_frequency', 'stat_market']\n",
    "station_view2 = station_data[cols_to_keep]\n",
    "station_view2 = station_view2[station_view2['stat_market'] != \"Not Applicable\"]\n",
    "station_view2['num_duplicates'] = station_view2.groupby(cols_to_keep)['stat_frequency'].transform('size')\n",
    "station_view2 = station_view2[station_view2['num_duplicates'] > 1]\n",
    "duplicate_proportion = len(station_view2) / len(station_data)\n",
    "print(f'Proportion of duplicate {cols_to_keep} (excluding \"Not Applicable\"): {duplicate_proportion:.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Each call sign has a unique frequency\n",
    "assert (station_data.groupby('stat_letters')['stat_frequency'].apply(lambda x: x.nunique()) == 1).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***First, matching based on (letters, frequency)***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "eng_data['eng_call_sign'] = eng_data['eng_call_sign'].str.strip()\n",
    "station_data['stat_letters'] = station_data['stat_letters'].str.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert station_data['stat_letters'].duplicated().sum() == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "before_count = eng_data['eng_call_sign'].duplicated().sum()\n",
    "\n",
    "def strip_FM(string):\n",
    "    if len(string) in [5, 6] and string.endswith('FM'):\n",
    "        return string[:-2]\n",
    "    return string\n",
    "\n",
    "eng_data.loc[:,'eng_call_sign'] = eng_data['eng_call_sign'].apply(strip_FM)\n",
    "\n",
    "assert eng_data['eng_call_sign'].duplicated().sum() == before_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Share of call signs in station data also in eng data: 84.44%\n",
      "Share of (call signs, frequency) pairs also in eng data: 78.44%\n"
     ]
    }
   ],
   "source": [
    "matching_series = station_data['stat_letters'].isin(eng_data['eng_call_sign'])\n",
    "print(f'Share of call signs in station data also in eng data: {matching_series.mean():.2%}')\n",
    "\n",
    "eng_data['call_sign_freq'] = tuple(zip(eng_data['eng_call_sign'], eng_data['eng_frequency']))\n",
    "station_data['call_sign_freq'] = tuple(zip(station_data['stat_letters'], station_data['stat_frequency']))\n",
    "\n",
    "matching_series = station_data['call_sign_freq'].isin(eng_data['call_sign_freq'])\n",
    "print(f'Share of (call signs, frequency) pairs also in eng data: {matching_series.mean():.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying state in station_data\n",
    "assert (station_data['stat_market'].apply(type) == str).all()\n",
    "station_data['stat_market_state'] = station_data['stat_market'].str.findall(r'([A-Z]{2})')\n",
    "station_data['stat_market_name'] = station_data['stat_market'].str.replace(r'[A-Z]{2}', '', regex=True).str.strip()\n",
    "assert (station_data[['stat_market_state','stat_market_name']].notna()).all().all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Splitting df\n",
    "station_data_match_df = station_data[station_data['call_sign_freq'].isin(eng_data['call_sign_freq'])].copy()\n",
    "station_data_match_df = station_data_match_df.merge(eng_data, left_on='call_sign_freq', right_on='call_sign_freq', how='left', validate='1:m')\n",
    "\n",
    "station_data_no_match_df = station_data[~station_data['call_sign_freq'].isin(eng_data['call_sign_freq'])].copy()\n",
    "\n",
    "eng_data_match_df = eng_data[eng_data['call_sign_freq'].isin(station_data['call_sign_freq'])].copy()\n",
    "eng_data_no_match_df = eng_data[~eng_data['call_sign_freq'].isin(station_data['call_sign_freq'])].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows in station_data_match_df: 4551\n",
      "Number of rows in station_data_no_match_df: 1211\n",
      "Number of rows in eng_data_match_df: 4551\n",
      "Number of rows in eng_data_no_match_df: 5233\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of rows in station_data_match_df: {len(station_data_match_df)}')\n",
    "print(f'Number of rows in station_data_no_match_df: {len(station_data_no_match_df)}')\n",
    "print(f'Number of rows in eng_data_match_df: {len(eng_data_match_df)}')\n",
    "print(f'Number of rows in eng_data_no_match_df: {len(eng_data_no_match_df)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Call Sign, Frequency) matches with mismatched state\n",
      "Portion of mismatches: 2.97%\n"
     ]
    }
   ],
   "source": [
    "def state_match_fn(row):\n",
    "\n",
    "    if row['stat_market_name'] == 'Not Applicable':\n",
    "        return True\n",
    "\n",
    "    elif row['eng_state'] in row['stat_market_state']:\n",
    "        return True\n",
    "\n",
    "    return False\n",
    "\n",
    "station_data_match_df['state_match'] = station_data_match_df.apply(state_match_fn, axis=1)\n",
    "\n",
    "print('(Call Sign, Frequency) matches with mismatched state')\n",
    "print(f'Portion of mismatches: {1 - station_data_match_df[\"state_match\"].mean():.2%}')\n",
    "\n",
    "## View the mismatches:\n",
    "# print()\n",
    "# for _, row in station_data_match_df.query(\"state_match == False\")[['stat_market_name', 'stat_market_state', 'eng_city', 'eng_state']].iterrows():\n",
    "#     print(list(row))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Second, matching based on (state, city, frequency) in eng_data and (frequency, market) in station_data***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number with market: 787\n",
      "number without market: 424\n"
     ]
    }
   ],
   "source": [
    "station_data_no_match_w_mark_df = station_data_no_match_df[station_data_no_match_df['stat_market'] != \"Not Applicable\"].copy()\n",
    "print(f'number with market: {len(station_data_no_match_w_mark_df)}')\n",
    "station_data_no_match_wout_mark_df = station_data_no_match_df[station_data_no_match_df['stat_market'] == \"Not Applicable\"].copy()\n",
    "print(f'number without market: {len(station_data_no_match_wout_mark_df)}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I will handle the case of unique (state, city, frequency) and (frequency, market). That is, I will exclude rows with duplicate (state, city, frequency) and (frequency, market) from the data sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of duplicates: 8\n",
      "percent dropped 1.02%\n"
     ]
    }
   ],
   "source": [
    "station_data_uniq = station_data_no_match_w_mark_df.copy()\n",
    "assert station_data_uniq['stat_letters'].duplicated().sum() == 0\n",
    "station_data_uniq['size'] = station_data_uniq.groupby(['stat_frequency', 'stat_market']).transform('size')\n",
    "print(f'number of duplicates: {len(station_data_uniq.query(\"size > 1\"))}')\n",
    "print(f'percent dropped {len(station_data_uniq.query(\"size > 1\")) / len(station_data_uniq):.2%}')\n",
    "station_data_uniq = station_data_uniq.query('size == 1').drop(columns='size').copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of eng_tuple pairs that map onto same call sign that will be dropped: 36\n",
      "percent dropped 0.69%\n"
     ]
    }
   ],
   "source": [
    "eng_data_uniq = eng_data_no_match_df.copy()\n",
    "\n",
    "eng_data_uniq['size_freq_loc'] = eng_data_uniq.groupby('eng_tuple').transform('size')\n",
    "eng_data_uniq['ID_freq_loc'] = eng_data_uniq.groupby('eng_tuple').ngroup()\n",
    "eng_data_uniq['size_call_sign'] = eng_data_uniq.groupby('eng_call_sign').transform('size')\n",
    "eng_data_uniq['ID_call_sign'] = eng_data_uniq.groupby('eng_call_sign').ngroup()\n",
    "\n",
    "print(f'Number of eng_tuple pairs that map onto same call sign that will be dropped: {len(eng_data_uniq.query(\"size_freq_loc != size_call_sign\"))}')\n",
    "print(f'percent dropped {len(eng_data_uniq.query(\"size_freq_loc != size_call_sign\")) / len(eng_data_uniq):.2%}')\n",
    "eng_data_uniq = eng_data_uniq.query('size_freq_loc == size_call_sign').drop(columns=['ID_freq_loc', 'ID_call_sign', 'size_freq_loc', 'call_sign_freq']).rename(columns={'size_call_sign': 'size'}).copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My goal is to link all rows in station_data_uniq (which represent unique stations) to a station in eng_data_uniq (which are identified by the freq, location tuple)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Portion of rows w/out frequency match dropped: 0.13%\n"
     ]
    }
   ],
   "source": [
    "# First, I will drop rows from station_data without a frequency match\n",
    "before_drop = len(station_data_uniq)\n",
    "\n",
    "station_data_uniq = station_data_uniq.query('stat_frequency in @eng_data_uniq[\"eng_frequency\"]').copy()\n",
    "print(f'Portion of rows w/out frequency match dropped: {(before_drop - len(station_data_uniq))/before_drop :.2%}')\n",
    "\n",
    "assert (station_data_uniq['stat_frequency'].isin(eng_data_uniq['eng_frequency'])).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now every row in station_data_uniq has a corresponding frequency in eng_data_uniq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import chain\n",
    "all_states = set(chain.from_iterable(station_data_uniq['stat_market_state']))\n",
    "all_frequencies = set(station_data_uniq['stat_frequency'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs_to_match = {}\n",
    "\n",
    "# gather all possible (frequency, state) pairs\n",
    "for frequency in all_frequencies:\n",
    "    station_view = station_data_uniq.query('stat_frequency == @frequency')[['stat_market', 'stat_market_name', 'stat_market_state']]\n",
    "    eng_view = eng_data_uniq.query('eng_frequency == @frequency')[['eng_city', 'eng_state', 'size']]\n",
    "\n",
    "    for state in all_states:\n",
    "        station_markets = station_view[station_view['stat_market_state'].apply(lambda x: state in x)][['stat_market', 'stat_market_name']].copy()    \n",
    "        eng_cities = eng_view.query('eng_state == @state')[['eng_city', 'size']].copy()\n",
    "\n",
    "        if not station_markets.empty:\n",
    "            dfs_to_match[(frequency, state)] = (eng_cities, station_markets)\n",
    "\n",
    "            \n",
    "assert sum([len(v[1]) for v in dfs_to_match.values()]) == len(station_data_uniq) + len(station_data_uniq[station_data_uniq['stat_market_state'].apply(len) == 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I need to consider the cases where a single station has multiple antennas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(104.3, 'CA')\n",
      "            eng_city  size\n",
      "20557    Los Angeles     2\n",
      "20595  Carmel Valley     1\n",
      "20627    Los Angeles     2\n",
      "20653       Oakhurst     1\n",
      "20688          Davis     1\n",
      "\n",
      "(105.5, 'AL')\n",
      "            eng_city  size\n",
      "22167  Muscle Shoals     2\n",
      "22393  Muscle Shoals     2\n",
      "\n",
      "(105.5, 'GA')\n",
      "        eng_city  size\n",
      "22350  Rossville     2\n",
      "22453  Rossville     2\n",
      "\n",
      "(105.1, 'NY')\n",
      "       eng_city  size\n",
      "21739  Fairport     1\n",
      "21770  New York     2\n",
      "21867  New York     2\n",
      "\n",
      "(107.3, 'NE')\n",
      "      eng_city  size\n",
      "24742  Lincoln     2\n",
      "24811  Lincoln     2\n",
      "\n",
      "(100.3, 'CA')\n",
      "          eng_city  size\n",
      "15466  Los Angeles     1\n",
      "15483     San Jose     2\n",
      "15525     San Jose     2\n",
      "15557  Bakersfield     1\n",
      "\n",
      "(92.3, 'KS')\n",
      "     eng_city  size\n",
      "5461   Newton     2\n",
      "5473   Newton     2\n",
      "\n",
      "(97.5, 'NY')\n",
      "            eng_city  size\n",
      "12039      Patchogue     2\n",
      "12066  Hoosick Falls     1\n",
      "12100      Patchogue     2\n",
      "\n",
      "(99.1, 'TX')\n",
      "      eng_city  size\n",
      "13882  Houston     2\n",
      "13949  Houston     2\n",
      "13990    Tyler     1\n",
      "14058   Odessa     1\n",
      "\n",
      "(102.9, 'IA')\n",
      "           eng_city  size\n",
      "18733  Cedar Rapids     2\n",
      "18922  Cedar Rapids     2\n",
      "\n",
      "(102.9, 'CA')\n",
      "            eng_city  size\n",
      "18717      San Diego     2\n",
      "18772      San Diego     2\n",
      "18839  Concord, etc.     1\n",
      "\n",
      "(103.3, 'ID')\n",
      "       eng_city  size\n",
      "19298  Caldwell     2\n",
      "19431  Caldwell     2\n",
      "\n",
      "(105.3, 'ME')\n",
      "      eng_city  size\n",
      "21996  Kittery     2\n",
      "22072  Kittery     2\n",
      "\n",
      "(106.1, 'CA')\n",
      "              eng_city  size\n",
      "23060           Arnold     1\n",
      "23078    San Francisco     2\n",
      "23101     Walnut Creek     1\n",
      "23165  Santa Margarita     1\n",
      "23175    San Francisco     2\n",
      "\n",
      "(106.5, 'CA')\n",
      "        eng_city  size\n",
      "23668  San Diego     2\n",
      "23726  San Diego     2\n",
      "23829   San Jose     1\n",
      "\n",
      "(107.9, 'CA')\n",
      "         eng_city  size\n",
      "25431  Sacramento     2\n",
      "25432       Chico     1\n",
      "25494  Sacramento     2\n",
      "\n",
      "(107.1, 'IN')\n",
      "       eng_city  size\n",
      "24378  Danville     2\n",
      "24721  Danville     2\n",
      "\n",
      "(101.1, 'AL')\n",
      "      eng_city  size\n",
      "16422  Cullman     2\n",
      "16610  Cullman     2\n",
      "\n",
      "(101.7, 'NM')\n",
      "         eng_city  size\n",
      "17308        Taos     1\n",
      "17315  Rio Rancho     2\n",
      "17336  Rio Rancho     2\n",
      "\n",
      "(101.7, 'WA')\n",
      "      eng_city  size\n",
      "17294  Prosser     2\n",
      "17379  Prosser     2\n",
      "\n",
      "(92.9, 'VA')\n",
      "     eng_city  size\n",
      "6347  Suffolk     2\n",
      "6441  Suffolk     2\n",
      "\n",
      "(92.9, 'TX')\n",
      "      eng_city  size\n",
      "6236  Pasadena     2\n",
      "6391    Jasper     1\n",
      "6417  Pasadena     2\n",
      "6437    Marlin     1\n",
      "\n",
      "(93.3, 'TX')\n",
      "         eng_city  size\n",
      "6702  Port Arthur     1\n",
      "6707      Killeen     2\n",
      "6713       Dallas     1\n",
      "6770  Haltom City     1\n",
      "6844      Killeen     2\n",
      "\n",
      "(93.7, 'MN')\n",
      "         eng_city  size\n",
      "7246  Minneapolis     2\n",
      "7322  Minneapolis     2\n",
      "\n",
      "(94.5, 'OH')\n",
      "       eng_city  size\n",
      "8265  Englewood     2\n",
      "8433  Englewood     2\n",
      "\n",
      "(94.5, 'NJ')\n",
      "     eng_city  size\n",
      "8278  Trenton     2\n",
      "8285  Trenton     2\n",
      "\n",
      "(94.5, 'TX')\n",
      "         eng_city  size\n",
      "8246  Gainesville     2\n",
      "8247  Gainesville     2\n",
      "8286      Houston     2\n",
      "8313      Houston     2\n",
      "\n",
      "(94.7, 'OK')\n",
      "           eng_city  size\n",
      "8612  Oklahoma City     2\n",
      "8637  Oklahoma City     2\n",
      "\n",
      "(94.9, 'OH')\n",
      "       eng_city  size\n",
      "8703  Fairfield     2\n",
      "8796  Fairfield     2\n",
      "8856        Ada     1\n",
      "\n",
      "(95.7, 'PA')\n",
      "          eng_city  size\n",
      "9737      Olyphant     1\n",
      "9817  Philadelphia     2\n",
      "9823  Philadelphia     2\n",
      "\n",
      "(95.7, 'FL')\n",
      "        eng_city  size\n",
      "9744  Clearwater     2\n",
      "9768  Clearwater     2\n",
      "\n",
      "(97.9, 'FL')\n",
      "            eng_city  size\n",
      "12531     Clearwater     2\n",
      "12536  St. Augustine     1\n",
      "12558    Tallahassee     1\n",
      "12642     Clearwater     2\n",
      "\n",
      "(101.5, 'PA')\n",
      "         eng_city  size\n",
      "16891   Covington     1\n",
      "16939  Waynesboro     2\n",
      "17058  Waynesboro     2\n",
      "\n",
      "(101.5, 'FL')\n",
      "             eng_city  size\n",
      "17034  St. Petersburg     2\n",
      "17036  St. Petersburg     2\n",
      "\n",
      "(99.3, 'PA')\n",
      "         eng_city  size\n",
      "14244    Franklin     2\n",
      "14266    Bradford     1\n",
      "14303  Harrisburg     1\n",
      "14311   Allentown     1\n",
      "14348    Franklin     2\n",
      "\n",
      "(99.3, 'NJ')\n",
      "            eng_city  size\n",
      "14120  Pleasantville     2\n",
      "14208  Pleasantville     2\n",
      "\n",
      "(101.9, 'CA')\n",
      "              eng_city  size\n",
      "17472          Redding     1\n",
      "17507         Glendale     2\n",
      "17558  Shingle Springs     1\n",
      "17621         Glendale     2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for k, v in dfs_to_match.items():\n",
    "    if (2 == v[0]['size']).any():\n",
    "        print(k)\n",
    "        print(v[0].head())\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fuzzywuzzy import fuzz\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import numpy as np\n",
    "\n",
    "def find_closest_match(market_df, city_df, threshold=70):\n",
    "    assert not market_df.empty\n",
    "\n",
    "    market_df = market_df.reset_index().rename(columns={'index': 'indices'})\n",
    "\n",
    "    maket_name_full_series = market_df.loc[:,'stat_market']\n",
    "    market_name_cut_series = market_df.loc[:,'stat_market_name']\n",
    "    market_index_series = market_df.loc[:,'indices'].astype(int)\n",
    "\n",
    "    city_df_size2 = (\n",
    "    city_df.query('size == 2')\n",
    "    .assign(indices=lambda df: df.index.map(int))\n",
    "    .groupby('eng_city', as_index=False)\n",
    "    .agg({'size': 'first', 'indices': lambda x: tuple(x)})\n",
    "    )\n",
    "    city_df_size1 = city_df.query('size == 1').reset_index().rename(columns={'index': 'indices'})\n",
    "    city_df_size1.indices = city_df_size1.indices.astype(int)\n",
    "    city_df_collapsed = pd.concat([city_df_size2, city_df_size1]).reset_index(drop=True)\n",
    "    city_df_collapsed.drop(columns='size', inplace=True)\n",
    "\n",
    "    city_name_series = city_df_collapsed.loc[:,'eng_city']\n",
    "    city_index_series = city_df_collapsed.loc[:,'indices']\n",
    "\n",
    "    if city_df.empty:\n",
    "        return {\n",
    "            (maket_name_full_series.iloc[i], market_index_series.iloc[i]): {\n",
    "                \"matched_entry\": None,\n",
    "                \"reason\": \"no cities to match with\"\n",
    "            }\n",
    "            for i in range(len(maket_name_full_series))\n",
    "        }\n",
    "\n",
    "    if len(market_df) == 1 and len(city_df) == 1:\n",
    "        return {\n",
    "            (maket_name_full_series.iloc[0], market_index_series.iloc[0]): {\n",
    "                \"matched_entry\": (city_name_series.iloc[0], city_index_series.iloc[0]),\n",
    "                \"reason\": 'one_to_one',\n",
    "            }\n",
    "        }\n",
    "\n",
    "    score_matrix = np.zeros((len(market_df), len(city_df)))\n",
    "\n",
    "    for i, market_name in enumerate(market_name_cut_series):\n",
    "        for j, city_name in enumerate(city_name_series):\n",
    "            score_matrix[i, j] = fuzz.ratio(market_name, city_name)\n",
    "\n",
    "    score_matrix[score_matrix < threshold] = 0\n",
    "\n",
    "    row_indices, col_indices = linear_sum_assignment(-score_matrix)\n",
    "\n",
    "    matches_within_series = {}\n",
    "\n",
    "    for row, col in zip(row_indices, col_indices):\n",
    "        market_name, market_index = maket_name_full_series.iloc[row], market_index_series.iloc[row]\n",
    "        city_name, city_index = city_name_series.iloc[col], city_index_series.iloc[col]\n",
    "        score = score_matrix[row, col]\n",
    "\n",
    "        if score >= threshold:\n",
    "            matches_within_series[(market_name, market_index)] = {\n",
    "                \"matched_entry\": (city_name, city_index),\n",
    "                \"reason\": score\n",
    "            }\n",
    "        else:\n",
    "            matches_within_series[(market_name, market_index)] = {\n",
    "                \"matched_entry\": None,\n",
    "                \"reason\": \"score lower than threshold\"\n",
    "            }\n",
    "\n",
    "    unmatched_rows = set(range(len(market_df))) - set(row_indices)\n",
    "\n",
    "    for row in unmatched_rows:\n",
    "        market_name, market_index = maket_name_full_series.iloc[row], maket_name_full_series.index[row]\n",
    "        matches_within_series[(market_name, market_index)] = { \n",
    "            \"matched_entry\": None,\n",
    "            \"reason\": \"no more cities to match with (markets < cities)\"\n",
    "        }\n",
    "\n",
    "    return matches_within_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "matches_dict = {}\n",
    "\n",
    "for key, value in dfs_to_match.items():\n",
    "    assert len(value) == 2\n",
    "    (eng_cities_df, station_market_df) = value\n",
    "    assert type(eng_cities_df) == pd.DataFrame and type(station_market_df) == pd.DataFrame\n",
    "\n",
    "    matches_with_scores = find_closest_match(station_market_df, eng_cities_df)\n",
    "    matches_dict[key] = matches_with_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a list of tuples to match\n",
    "city_market_matches = []\n",
    "for key, value in matches_dict.items():\n",
    "    for (market_name, market_index), matched_entry_dict in value.items():\n",
    "        if matched_entry_dict['matched_entry'] is None:\n",
    "            continue\n",
    "        (city_name, city_index) = matched_entry_dict['matched_entry']\n",
    "        reason = matched_entry_dict['reason']\n",
    "        city_market_matches.append((key, market_name, market_index, city_name, city_index, reason))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>market</th>\n",
       "      <th>market_index</th>\n",
       "      <th>city</th>\n",
       "      <th>city_index</th>\n",
       "      <th>match_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>(102.3, NC)</td>\n",
       "      <td>Fayetteville NC</td>\n",
       "      <td>128508</td>\n",
       "      <td>Lumberton</td>\n",
       "      <td>17922</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>(102.3, CT)</td>\n",
       "      <td>New London CT</td>\n",
       "      <td>99533</td>\n",
       "      <td>Stonington</td>\n",
       "      <td>18019</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>(102.3, NH)</td>\n",
       "      <td>Manchester NH</td>\n",
       "      <td>182829</td>\n",
       "      <td>Concord</td>\n",
       "      <td>18010</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>(102.3, IA)</td>\n",
       "      <td>Dubuque IA</td>\n",
       "      <td>86543</td>\n",
       "      <td>Dubuque</td>\n",
       "      <td>18228</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>(102.3, IN)</td>\n",
       "      <td>Ft. Wayne IN</td>\n",
       "      <td>110905</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>17898</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           key           market  market_index        city city_index  \\\n",
       "0  (102.3, NC)  Fayetteville NC        128508   Lumberton      17922   \n",
       "1  (102.3, CT)    New London CT         99533  Stonington      18019   \n",
       "2  (102.3, NH)    Manchester NH        182829     Concord      18010   \n",
       "3  (102.3, IA)       Dubuque IA         86543     Dubuque      18228   \n",
       "4  (102.3, IN)     Ft. Wayne IN        110905      Auburn      17898   \n",
       "\n",
       "  match_reason  \n",
       "0   one_to_one  \n",
       "1   one_to_one  \n",
       "2   one_to_one  \n",
       "3        100.0  \n",
       "4   one_to_one  "
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "city_market_matches_df = pd.DataFrame(city_market_matches, columns=['key','market', 'market_index', 'city', 'city_index', 'match_reason'])\n",
    "city_market_matches_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>market</th>\n",
       "      <th>market_index</th>\n",
       "      <th>city</th>\n",
       "      <th>city_index</th>\n",
       "      <th>match_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>331</th>\n",
       "      <td>(101.5, IA)</td>\n",
       "      <td>Omaha-Council Bluffs NE-IA</td>\n",
       "      <td>33695</td>\n",
       "      <td>Decorah</td>\n",
       "      <td>17060</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>332</th>\n",
       "      <td>(101.5, NE)</td>\n",
       "      <td>Omaha-Council Bluffs NE-IA</td>\n",
       "      <td>33695</td>\n",
       "      <td>Hastings</td>\n",
       "      <td>16973</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>(101.3, IA)</td>\n",
       "      <td>Quad Cities (Davenport-Rock Island-Moline IA-IL)</td>\n",
       "      <td>78829</td>\n",
       "      <td>Creston</td>\n",
       "      <td>16681</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>(101.3, IL)</td>\n",
       "      <td>Quad Cities (Davenport-Rock Island-Moline IA-IL)</td>\n",
       "      <td>78829</td>\n",
       "      <td>East Moline</td>\n",
       "      <td>16772</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>(103.3, VT)</td>\n",
       "      <td>Burlington-Plattsburgh VT-NY</td>\n",
       "      <td>163381</td>\n",
       "      <td>Waterbury</td>\n",
       "      <td>19445</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             key                                            market  \\\n",
       "331  (101.5, IA)                        Omaha-Council Bluffs NE-IA   \n",
       "332  (101.5, NE)                        Omaha-Council Bluffs NE-IA   \n",
       "81   (101.3, IA)  Quad Cities (Davenport-Rock Island-Moline IA-IL)   \n",
       "83   (101.3, IL)  Quad Cities (Davenport-Rock Island-Moline IA-IL)   \n",
       "137  (103.3, VT)                      Burlington-Plattsburgh VT-NY   \n",
       "\n",
       "     market_index         city city_index match_reason  \n",
       "331         33695      Decorah      17060   one_to_one  \n",
       "332         33695     Hastings      16973   one_to_one  \n",
       "81          78829      Creston      16681   one_to_one  \n",
       "83          78829  East Moline      16772   one_to_one  \n",
       "137        163381    Waterbury      19445   one_to_one  "
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "assert city_market_matches_df.duplicated(subset=['city_index']).sum() == 0\n",
    "\n",
    "# Drop rows of markets in two states that have perfect matches in both states\n",
    "multiple_matches = city_market_matches_df[city_market_matches_df.duplicated(subset=['market_index'], keep=False)].sort_values(by='market_index')\n",
    "assert multiple_matches['match_reason'].eq('one_to_one').all()\n",
    "city_market_matches_df = city_market_matches_df[city_market_matches_df.index.isin(multiple_matches.index) == False].copy()\n",
    "\n",
    "assert city_market_matches_df.duplicated(subset=['market_index']).sum() == 0\n",
    "\n",
    "multiple_matches.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tuple rows to expand:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>key</th>\n",
       "      <th>market</th>\n",
       "      <th>market_index</th>\n",
       "      <th>city</th>\n",
       "      <th>city_index</th>\n",
       "      <th>match_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>(105.5, AL)</td>\n",
       "      <td>Florence-Muscle Shoals AL</td>\n",
       "      <td>214751</td>\n",
       "      <td>Muscle Shoals</td>\n",
       "      <td>(22167, 22393)</td>\n",
       "      <td>74.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>(105.1, NY)</td>\n",
       "      <td>New York NY</td>\n",
       "      <td>207593</td>\n",
       "      <td>New York</td>\n",
       "      <td>(21770, 21867)</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>69</th>\n",
       "      <td>(107.3, NE)</td>\n",
       "      <td>Lincoln NE</td>\n",
       "      <td>5476</td>\n",
       "      <td>Lincoln</td>\n",
       "      <td>(24742, 24811)</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>(102.9, IA)</td>\n",
       "      <td>Cedar Rapids IA</td>\n",
       "      <td>90926</td>\n",
       "      <td>Cedar Rapids</td>\n",
       "      <td>(18733, 18922)</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>(102.9, CA)</td>\n",
       "      <td>San Diego CA</td>\n",
       "      <td>43606</td>\n",
       "      <td>San Diego</td>\n",
       "      <td>(18717, 18772)</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             key                     market  market_index           city  \\\n",
       "35   (105.5, AL)  Florence-Muscle Shoals AL        214751  Muscle Shoals   \n",
       "40   (105.1, NY)                New York NY        207593       New York   \n",
       "69   (107.3, NE)                 Lincoln NE          5476        Lincoln   \n",
       "122  (102.9, IA)            Cedar Rapids IA         90926   Cedar Rapids   \n",
       "125  (102.9, CA)               San Diego CA         43606      San Diego   \n",
       "\n",
       "         city_index match_reason  \n",
       "35   (22167, 22393)         74.0  \n",
       "40   (21770, 21867)        100.0  \n",
       "69   (24742, 24811)        100.0  \n",
       "122  (18733, 18922)        100.0  \n",
       "125  (18717, 18772)        100.0  "
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print('tuple rows to expand:')\n",
    "city_market_matches_df[city_market_matches_df['city_index'].apply(lambda x: isinstance(x, tuple))].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "# expand tuples\n",
    "city_market_matches_df = city_market_matches_df.explode('city_index').reset_index(drop=True)\n",
    "assert city_market_matches_df['city_index'].apply(lambda x: isinstance(x, (np.int64, int))).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [],
   "source": [
    "city_market_matches_df = city_market_matches_df.merge(station_data_uniq, left_on=['market_index'], right_index=True, how='left', validate='m:1')\n",
    "\n",
    "city_market_matches_df = city_market_matches_df.merge(eng_data_uniq, left_on=['city_index'], right_index=True, how='left', validate='1:1')\n",
    "city_market_matches_df.drop(columns=['market', 'market_index', 'city', 'city_index', 'eng_tuple', 'key'], inplace=True)\n",
    "\n",
    "cols_to_front = ['stat_market_name', 'eng_city', 'stat_market_state', 'eng_state', 'stat_frequency', 'eng_frequency']\n",
    "city_market_matches_df = city_market_matches_df[cols_to_front + [col for col in city_market_matches_df.columns if col not in cols_to_front]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert (city_market_matches_df['stat_frequency'] == city_market_matches_df['eng_frequency']).all()\n",
    "for row in city_market_matches_df.itertuples():\n",
    "    assert row.eng_state in row.stat_market_state "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'size', 'match_reason'}\n",
      "{'eng_tuple', 'state_match'}\n"
     ]
    }
   ],
   "source": [
    "new_matches_column_names = set(city_market_matches_df.columns)\n",
    "old_matches_column_names = set(station_data_match_df.columns)\n",
    "common_columns = sorted(list(new_matches_column_names.intersection(old_matches_column_names))) + ['match_reason']\n",
    "print(new_matches_column_names.difference(old_matches_column_names))\n",
    "print(old_matches_column_names.difference(new_matches_column_names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter both DataFrames to include only the common columns\n",
    "df1 = city_market_matches_df[common_columns]\n",
    "station_data_match_df['match_reason'] = 'letter_match'\n",
    "df2 = station_data_match_df[common_columns]\n",
    "\n",
    "# Concatenate the DataFrames along rows\n",
    "matched_data = pd.concat([df1, df2], axis=0)\n",
    "matched_data.drop(columns='call_sign_freq', inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>eng_call_sign</th>\n",
       "      <th>eng_city</th>\n",
       "      <th>eng_erp</th>\n",
       "      <th>eng_frequency</th>\n",
       "      <th>eng_latitude</th>\n",
       "      <th>eng_longitude</th>\n",
       "      <th>eng_rcamsl</th>\n",
       "      <th>eng_state</th>\n",
       "      <th>stat_biamarketid</th>\n",
       "      <th>stat_biastationcode</th>\n",
       "      <th>...</th>\n",
       "      <th>stat_frequency</th>\n",
       "      <th>stat_letters</th>\n",
       "      <th>stat_market</th>\n",
       "      <th>stat_market_name</th>\n",
       "      <th>stat_market_state</th>\n",
       "      <th>stat_rank</th>\n",
       "      <th>stat_rating</th>\n",
       "      <th>stat_startdate</th>\n",
       "      <th>stat_supermarket</th>\n",
       "      <th>match_reason</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>WJSK</td>\n",
       "      <td>Lumberton</td>\n",
       "      <td>3.00</td>\n",
       "      <td>102.3</td>\n",
       "      <td>34.599444</td>\n",
       "      <td>-79.009167</td>\n",
       "      <td>122.0</td>\n",
       "      <td>NC</td>\n",
       "      <td>136.0</td>\n",
       "      <td>15938.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.3</td>\n",
       "      <td>WFNC</td>\n",
       "      <td>Fayetteville NC</td>\n",
       "      <td>Fayetteville</td>\n",
       "      <td>[NC]</td>\n",
       "      <td>129.0</td>\n",
       "      <td>0.50</td>\n",
       "      <td>1964.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>WVVE</td>\n",
       "      <td>Stonington</td>\n",
       "      <td>3.00</td>\n",
       "      <td>102.3</td>\n",
       "      <td>41.406389</td>\n",
       "      <td>-71.837500</td>\n",
       "      <td>137.0</td>\n",
       "      <td>CT</td>\n",
       "      <td>262.0</td>\n",
       "      <td>17132.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.3</td>\n",
       "      <td>WAXK</td>\n",
       "      <td>New London CT</td>\n",
       "      <td>New London</td>\n",
       "      <td>[CT]</td>\n",
       "      <td>171.0</td>\n",
       "      <td>4.15</td>\n",
       "      <td>1981.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>WKXL</td>\n",
       "      <td>Concord</td>\n",
       "      <td>3.00</td>\n",
       "      <td>102.3</td>\n",
       "      <td>43.216667</td>\n",
       "      <td>-71.576111</td>\n",
       "      <td>222.0</td>\n",
       "      <td>NH</td>\n",
       "      <td>159.0</td>\n",
       "      <td>16296.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.3</td>\n",
       "      <td>WOTX</td>\n",
       "      <td>Manchester NH</td>\n",
       "      <td>Manchester</td>\n",
       "      <td>[NH]</td>\n",
       "      <td>187.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1972.0</td>\n",
       "      <td>Portsmouth-Rochester-Dover NH-ME</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>KGGY</td>\n",
       "      <td>Dubuque</td>\n",
       "      <td>1.65</td>\n",
       "      <td>102.3</td>\n",
       "      <td>42.541111</td>\n",
       "      <td>-90.612778</td>\n",
       "      <td>374.0</td>\n",
       "      <td>IA</td>\n",
       "      <td>200.0</td>\n",
       "      <td>11903.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.3</td>\n",
       "      <td>KXGE</td>\n",
       "      <td>Dubuque IA</td>\n",
       "      <td>Dubuque</td>\n",
       "      <td>[IA]</td>\n",
       "      <td>225.0</td>\n",
       "      <td>5.15</td>\n",
       "      <td>1980.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WGL</td>\n",
       "      <td>Auburn</td>\n",
       "      <td>3.00</td>\n",
       "      <td>102.3</td>\n",
       "      <td>41.333611</td>\n",
       "      <td>-85.052222</td>\n",
       "      <td>355.0</td>\n",
       "      <td>IN</td>\n",
       "      <td>108.0</td>\n",
       "      <td>15601.0</td>\n",
       "      <td>...</td>\n",
       "      <td>102.3</td>\n",
       "      <td>WCKZ</td>\n",
       "      <td>Ft. Wayne IN</td>\n",
       "      <td>Ft. Wayne</td>\n",
       "      <td>[IN]</td>\n",
       "      <td>103.0</td>\n",
       "      <td>0.20</td>\n",
       "      <td>1967.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>one_to_one</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows  23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  eng_call_sign    eng_city  eng_erp  eng_frequency  eng_latitude  \\\n",
       "0          WJSK   Lumberton     3.00          102.3     34.599444   \n",
       "1          WVVE  Stonington     3.00          102.3     41.406389   \n",
       "2          WKXL     Concord     3.00          102.3     43.216667   \n",
       "3          KGGY     Dubuque     1.65          102.3     42.541111   \n",
       "4           WGL      Auburn     3.00          102.3     41.333611   \n",
       "\n",
       "   eng_longitude  eng_rcamsl eng_state  stat_biamarketid  stat_biastationcode  \\\n",
       "0     -79.009167       122.0        NC             136.0              15938.0   \n",
       "1     -71.837500       137.0        CT             262.0              17132.0   \n",
       "2     -71.576111       222.0        NH             159.0              16296.0   \n",
       "3     -90.612778       374.0        IA             200.0              11903.0   \n",
       "4     -85.052222       355.0        IN             108.0              15601.0   \n",
       "\n",
       "   ... stat_frequency stat_letters      stat_market  stat_market_name  \\\n",
       "0  ...          102.3         WFNC  Fayetteville NC      Fayetteville   \n",
       "1  ...          102.3         WAXK    New London CT        New London   \n",
       "2  ...          102.3         WOTX    Manchester NH        Manchester   \n",
       "3  ...          102.3         KXGE       Dubuque IA           Dubuque   \n",
       "4  ...          102.3         WCKZ     Ft. Wayne IN         Ft. Wayne   \n",
       "\n",
       "  stat_market_state stat_rank stat_rating stat_startdate  \\\n",
       "0              [NC]     129.0        0.50         1964.0   \n",
       "1              [CT]     171.0        4.15         1981.0   \n",
       "2              [NH]     187.0         NaN         1972.0   \n",
       "3              [IA]     225.0        5.15         1980.0   \n",
       "4              [IN]     103.0        0.20         1967.0   \n",
       "\n",
       "                   stat_supermarket  match_reason  \n",
       "0                               NaN    one_to_one  \n",
       "1                               NaN    one_to_one  \n",
       "2  Portsmouth-Rochester-Dover NH-ME    one_to_one  \n",
       "3                               NaN         100.0  \n",
       "4                               NaN    one_to_one  \n",
       "\n",
       "[5 rows x 23 columns]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['eng_call_sign', 'eng_city', 'eng_erp', 'eng_frequency', 'eng_latitude',\n",
       "       'eng_longitude', 'eng_rcamsl', 'eng_state', 'stat_biamarketid',\n",
       "       'stat_biastationcode', 'stat_format1', 'stat_format2', 'stat_format3',\n",
       "       'stat_frequency', 'stat_letters', 'stat_market', 'stat_market_name',\n",
       "       'stat_market_state', 'stat_rank', 'stat_rating', 'stat_startdate',\n",
       "       'stat_supermarket', 'match_reason'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 177,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "match_reason\n",
       "letter_match    4551\n",
       "one_to_one       289\n",
       "100.0             59\n",
       "74.0               3\n",
       "71.0               3\n",
       "79.0               1\n",
       "80.0               1\n",
       "90.0               1\n",
       "72.0               1\n",
       "92.0               1\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 178,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "matched_data.match_reason.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rows successfully matched: 87.41%\n"
     ]
    }
   ],
   "source": [
    "print(f'rows successfully matched: {len(matched_data)/len(station_data):.2%}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_data.to_csv('Exports/Data/3.1997StationDatawAntennaInfo.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "station_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
